{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint8 機械学習スクラッチ アンサンブル学習\n",
    "\n",
    "## 1.このSprintについて\n",
    "\n",
    "### Sprintの目的\n",
    "- アンサンブル学習について理解する\n",
    "### どのように学ぶか\n",
    "スクラッチでアンサンブル学習の各種手法を実装していきます。\n",
    "\n",
    "## 2.アンサンブル学習\n",
    "3種類のアンサンブル学習をスクラッチ実装していきます。そして、それぞれの効果を小さめのデータセットで確認します。\n",
    "\n",
    "- ブレンディング\n",
    "- バギング\n",
    "- スタッキング\n",
    "\n",
    "### 小さなデータセットの用意\n",
    "以前も利用した回帰のデータセットを用意します。\n",
    "\n",
    "[House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)\n",
    "\n",
    "この中のtrain.csvをダウンロードし、目的変数としてSalePrice、説明変数として、GrLivAreaとYearBuiltを使います。\n",
    "\n",
    "train.csvを学習用（train）8割、検証用（val）2割に分割してください。\n",
    "\n",
    "### scikit-learn\n",
    "単一のモデルはスクラッチ実装ではなく、scikit-learnなどのライブラリの使用を推奨します。\n",
    "\n",
    "[sklearn.linear_model.LinearRegression — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "\n",
    "[sklearn.svm.SVR — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)\n",
    "\n",
    "[sklearn.tree.DecisionTreeRegressor — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "X = df[[\"GrLivArea\", \"YearBuilt\"]].values\n",
    "y = df[\"SalePrice\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "線形回帰 MSE : 2495554898.6683216\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_y_pred = lr.predict(X_test)\n",
    "print(\"線形回帰 MSE : {}\".format(mean_squared_error(y_test, lr_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM MSE : 7861854841.842987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "svm = SVR()\n",
    "svm.fit(X_train, y_train)\n",
    "svm_y_pred = svm.predict(X_test)\n",
    "print(\"SVM MSE : {}\".format(mean_squared_error(y_test, svm_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "決定木 MSE : 2204076821.5252094\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(X_train, y_train)\n",
    "tree_y_pred = tree.predict(X_test)\n",
    "print(\"決定木 MSE : {}\".format(mean_squared_error(y_test, tree_y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.ブレンディング\n",
    "＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿\n",
    "### 【問題1】ブレンディングのスクラッチ実装\n",
    "ブレンディング をスクラッチ実装し、単一モデルより精度があがる例を 最低3つ 示してください。精度があがるとは、検証用データに対する平均二乗誤差（MSE）が小さくなることを指します。\n",
    "\n",
    "### ブレンディングとは\n",
    "ブレンディングとは、N個の多様なモデルを独立して学習させ、推定結果を重み付けした上で足し合わせる方法です。最も単純には平均をとります。多様なモデルとは、以下のような条件を変化させることで作り出すものです。\n",
    "\n",
    "- 手法（例：線形回帰、SVM、決定木、ニューラルネットワークなど）\n",
    "- ハイパーパラメータ（例：SVMのカーネルの種類、重みの初期値など）\n",
    "- 入力データの前処理の仕方（例：標準化、対数変換、PCAなど）\n",
    "\n",
    "重要なのはそれぞれのモデルが大きく異なることです。\n",
    "\n",
    "回帰問題でのブレンディングは非常に単純であるため、scikit-learnには用意されていません。\n",
    "\n",
    "#### 《補足》\n",
    "\n",
    "分類問題の場合は、多数決を行います。回帰問題に比べると複雑なため、scikit-learnにはVotingClassifierが用意されています。\n",
    "\n",
    "[sklearn.ensemble.VotingClassifier — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend(X_train, X_test, y_train, model1, model2):\n",
    "    model1.fit(X_train, y_train)\n",
    "    model1_pred = model1.predict(X_test)\n",
    "    \n",
    "    model2.fit(X_train, y_train)\n",
    "    model2_pred = model2.predict(X_test)\n",
    "    y_pred = (model1_pred + model2_pred) / 2\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   線形回帰     MSE : 2495554898.6683216\n",
      "      SVM       MSE : 7861854841.842987\n",
      "    決定木      MSE : 2204076821.5252094\n",
      "線形回帰+決定木 MSE : 1861935310.537359\n"
     ]
    }
   ],
   "source": [
    "# 線形回帰と決定木の予測結果の平均を予測値とする\n",
    "y_pred = blend(X_train, X_test, y_train, lr, tree)\n",
    "print(\"   線形回帰     MSE : {}\".format(mean_squared_error(y_test, lr_y_pred)))\n",
    "print(\"      SVM       MSE : {}\".format(mean_squared_error(y_test, svm_y_pred)))\n",
    "print(\"    決定木      MSE : {}\".format(mean_squared_error(y_test, tree_y_pred)))\n",
    "print(\"線形回帰+決定木 MSE : {}\".format(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "線形回帰   MSE : 2495554898.6683216\n",
      "SVM        MSE : 7861854841.842987\n",
      "決定木     MSE : 2204076821.5252094\n",
      "SVM+決定木 MSE : 3390320870.5819483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# SVMと決定木の予測結果の平均を予測値とする\n",
    "y_pred = blend(X_train, X_test, y_train, svm, tree)\n",
    "print(\"線形回帰   MSE : {}\".format(mean_squared_error(y_test, lr_y_pred)))\n",
    "print(\"SVM        MSE : {}\".format(mean_squared_error(y_test, svm_y_pred)))\n",
    "print(\"決定木     MSE : {}\".format(mean_squared_error(y_test, tree_y_pred)))\n",
    "print(\"SVM+決定木 MSE : {}\".format(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   線形回帰     MSE : 2495554898.6683216\n",
      "      SVM       MSE : 7861854841.842987\n",
      "    決定木      MSE : 2204076821.5252094\n",
      "SVM 線形+多項式 MSE : 4450951532.463163\n"
     ]
    }
   ],
   "source": [
    "# SVMの線形カーネルと多項式カーネルのモデルの予測の平均\n",
    "model1 = SVR(gamma='scale', kernel='linear')\n",
    "model2 = SVR(gamma='scale') \n",
    "y_pred = blend(X_train, X_test, y_train, model1, model2)\n",
    "print(\"   線形回帰     MSE : {}\".format(mean_squared_error(y_test, lr_y_pred)))\n",
    "print(\"      SVM       MSE : {}\".format(mean_squared_error(y_test, svm_y_pred)))\n",
    "print(\"    決定木      MSE : {}\".format(mean_squared_error(y_test, tree_y_pred)))\n",
    "print(\"SVM 線形+多項式 MSE : {}\".format(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.バギング\n",
    "＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿\n",
    "### 【問題2】バギングのスクラッチ実装\n",
    "バギング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。\n",
    "\n",
    "### バギングとは\n",
    "バギングは入力データの選び方を多様化する方法です。学習データから重複を許した上でランダムに抜き出すことで、N種類のサブセット（ ブートストラップサンプル ）を作り出します。それらによってモデルをN個学習し、推定結果の平均をとります。ブレンディングと異なり、それぞれの重み付けを変えることはありません。\n",
    "\n",
    "[sklearn.model_selection.train_test_split — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "scikit-learnのtrain_test_splitを、shuffleパラメータをTrueにして使うことで、ランダムにデータを分割することができます。これによりブートストラップサンプルが手に入ります。\n",
    "\n",
    "推定結果の平均をとる部分はブースティングと同様の実装になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    決定木      MSE : 2204076821.5252094\n",
      "bagging 決定木  MSE : 2159302214.8116994\n"
     ]
    }
   ],
   "source": [
    "def bagging(X_train, X_test, y_train, model, n=2):\n",
    "    y_pred = np.zeros(X_test.shape[0])\n",
    "    for i in range(n):\n",
    "        X_divided = train_test_split(X_train, random_state=random.randint(0, i))[0]\n",
    "        y_divided = train_test_split(X_train, random_state=random.randint(0, i))[0]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred += model.predict(X_test)\n",
    "    y_pred = y_pred / n\n",
    "    return y_pred\n",
    "\n",
    "y_pred = bagging(X_train, X_test, y_train, tree, n=100)\n",
    "\n",
    "print(\"    決定木      MSE : {}\".format(mean_squared_error(y_test, tree_y_pred)))\n",
    "print(\"bagging 決定木  MSE : {}\".format(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.スタッキング\n",
    "＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿＿\n",
    "### 【問題3】スタッキングのスクラッチ実装\n",
    "スタッキング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。\n",
    "\n",
    "### スタッキングとは\n",
    "スタッキングの手順は以下の通りです。最低限ステージ0とステージ1があればスタッキングは成立するため、それを実装してください。まずは \n",
    "$K_0 = 3, M_0 = 2$ 程度にします。\n",
    "\n",
    "#### 《学習時》\n",
    "\n",
    "#### （ステージ 0 ）\n",
    "\n",
    "- 学習データを $K_0$ 個に分割する。\n",
    "- 分割した内の $(K_0 − 1)$ 個をまとめて学習用データ、残り 1 個を推定用データとする組み合わせが $K_0$ 個作れる。\n",
    "- あるモデルのインスタンスを $K_0$ 個用意し、異なる学習用データを使い学習する。\n",
    "- それぞれの学習済みモデルに対して、使っていない残り 1 個の推定用データを入力し、推定値を得る。（これをブレンドデータと呼ぶ）\n",
    "- さらに、異なるモデルのインスタンスも $K_0$ 個用意し、同様のことを行う。モデルが $M_0$ 個あれば、$M_0$ 個のブレンドデータが得られる。\n",
    "\n",
    "#### （ステージ n ）\n",
    "\n",
    "- ステージ $n −1 $ のブレンドデータを$M_n − 1$ 次元の特徴量を持つ学習用データと考え、$K_n$ 個に分割する。以下同様である。\n",
    "\n",
    "#### （ステージ N ）＊最後のステージ\n",
    "\n",
    "- ステージ$N − 1$ の$M_{N-1}$ 個のブレンドデータを$M_{N−1}$ 次元の特徴量の入力として、1種類のモデルの学習を行う。これが最終的な推定を行うモデルとなる。\n",
    "\n",
    "#### 《推定時》\n",
    "\n",
    "#### （ステージ 0 ）\n",
    "\n",
    "- テストデータを $K_0×M_0$ 個の学習済みモデルに入力し、$K_0×M_0$ 個の推定値を得る。これを$K_0$ の軸で平均値を求め $M_0$ 次元の特徴量を持つデータを得る。（ブレンドテストと呼ぶ）\n",
    "\n",
    "#### （ステージ n ）\n",
    "\n",
    "- ステージ $n−1$ で得たブレンドテストを$K_n×M_n$ 個の学習済みモデルに入力し、$K_n×M_n$ 個の推定値を得る。これを_\\K_n$ の軸で平均値を求め $M_0 次元の特徴量を持つデータを得る。（ブレンドテストと呼ぶ）\n",
    "\n",
    "#### （ステージ N ）＊最後のステージ\n",
    "\n",
    "- ステージ$N−1$ で得たブレンドテストを学習済みモデルに入力し、推定値を得る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stacking():\n",
    "    def __init__(self, split_n=3, model_n=2):\n",
    "        self.split_n = split_n\n",
    "        self.model_n = model_n\n",
    "\n",
    "    def fit(self, X_train, y_train, X_test, y_test, models):\n",
    "        # K個に分割するdividerを作る\n",
    "        divider = np.zeros(self.split_n)\n",
    "        vol = X_train.shape[0]\n",
    "        num = self.split_n\n",
    "        for i in range(self.split_n):\n",
    "            divider[i] = math.ceil(vol/num)\n",
    "            num -= 1\n",
    "            vol = vol-divider[i]\n",
    "        \n",
    "        self.divider = divider.astype(int)\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.models = models\n",
    "        print(self.divider)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        for m in range(self.model_n):\n",
    "            divide_point = 0\n",
    "            for n in range(self.split_n):\n",
    "                idx = np.zeros(X_train.shape[0], dtype=bool)\n",
    "                idx[divide_point:divide_point+self.divider[n]]= True\n",
    "                self.X_test_divided = X_train[idx, :]\n",
    "                self.X_train_divided = X_train[~idx, :]\n",
    "                self.y_test_divided = y_train[idx]\n",
    "                self.y_train_divided = y_train[~idx]                    \n",
    "                \n",
    "                models[m].fit(self.X_train_divided, self.y_train_divided)\n",
    "                if n == 0:\n",
    "                    blend = models[m].predict(self.X_test_divided)\n",
    "                    pred_data = models[m].predict(X_test)\n",
    "                else:\n",
    "                    blend = np.r_[blend, models[m].predict(self.X_test_divided)]\n",
    "                    pred_data = np.c_[pred_data, models[m].predict(X_test)]\n",
    "            \n",
    "                divide_point += self.divider[n]\n",
    "            if m ==0:\n",
    "                blend_data =blend.reshape(-1, 1)\n",
    "                blend_pred_data = np.mean(pred_data, axis=1)\n",
    "            else:\n",
    "                blend_data = np.c_[blend_data, blend.reshape(-1, 1) ]\n",
    "                blend_pred_data = np.c_[blend_pred_data, np.mean(pred_data, axis=1)]\n",
    "        \n",
    "        models[0].fit(blend_data, y_train)\n",
    "        y_pred = models[0].predict(blend_pred_data)                               \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[390 389 389]\n",
      "線形回帰 MSE : 2495554898.6683216\n",
      "SVM      MSE : 7861854841.842987\n",
      "決定木   MSE : 2204076821.5252094\n",
      "Stacking MSE : 1932374160.447718\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "models = [LinearRegression(), DecisionTreeRegressor()]\n",
    "\n",
    "stacking = Stacking()\n",
    "stacking.fit(X_train, y_train, X_test, y_test, models)\n",
    "y_pred = stacking.predict(X_test)\n",
    "\n",
    "print(\"線形回帰 MSE : {}\".format(mean_squared_error(y_test, lr_y_pred)))\n",
    "print(\"SVM      MSE : {}\".format(mean_squared_error(y_test, svm_y_pred)))\n",
    "print(\"決定木   MSE : {}\".format(mean_squared_error(y_test, tree_y_pred)))\n",
    "print(\"Stacking MSE : {}\".format(mean_squared_error(y_test, y_pred)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
