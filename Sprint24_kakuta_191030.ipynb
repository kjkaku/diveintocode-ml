{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Sprint24_kakuta_191030.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"tRLm_iF2cuPi","colab_type":"text"},"source":["# Sprint24 Seq2Seq\n","\n","## 1.このSprintについて\n","\n","### Sprintの目的\n","- 系列データに関する応用例を学ぶ\n","\n","### どのように学ぶか\n","- 公開されているコードを元に学んでいきます。\n","\n","## 2.機械翻訳\n","系列データに関する手法の基本的な活用例としては機械翻訳があります。これは系列データを入力し、系列データを出力する Sequence to Sequence の手法によって行えます。"]},{"cell_type":"markdown","metadata":{"id":"oWaJmj6OcuPm","colab_type":"text"},"source":["### 【問題1】機械翻訳の実行とコードリーディング\n","Keras公式のサンプルコードで、短い英語からフランス語への変換を行うものが公開されています。これを動かしてください。\n","\n","[keras/lstm_seq2seq.py at master · keras-team/keras](https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py)\n","\n","その上でこのサンプルコードの各部分がどういった役割かを読み取り、まとめてください。以下のようにどこからどこの行が何をしているかを記述してください。\n","\n","#### （例）\n","\n","- 51から55行目 : ライブラリのimport\n","- 57から62行目 : ハイパーパラメータの設定\n","\n","#### 《文字単位のトークン化》\n","\n","この実装ではテキストのベクトル化の際に、単語ではなく文字ごとを1つのトークンとして扱っています。\n","\n","scikit-learnでBoWを計算するCountVectorizerの場合では、デフォルトの引数はanalyzer=’word’で単語を扱いますが、charやchar_wbとすることで文字を扱えるようになります。\n","\n","charとchar_wbの2種類の方法があり、char_wbを指定した場合、n_gramが単語内からのみ作成されます。逆にcharは単語の区切りが関係なくn_gramが作成されます。This movie is very good.というテキストを3-gramでカウントする時、charではs mやe iといった単語をまたぐ数え方もしますが、char_wbではこれらを見ません。\n","\n","[sklearn.feature_extraction.text.CountVectorizer — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"]},{"cell_type":"markdown","metadata":{"id":"8rtG-O4KcuPn","colab_type":"text"},"source":["コメントで記述"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"gwG5bttFcuPo","colab_type":"code","outputId":"76fd4e36-325d-4964-d85e-0fc0734f9a54","colab":{}},"source":["'''\n","**Data download**\n","[English to French sentence pairs.\n","](http://www.manythings.org/anki/fra-eng.zip)\n","[Lots of neat sentence pairs datasets.\n","](http://www.manythings.org/anki/)\n","**References**\n","- [Sequence to Sequence Learning with Neural Networks\n","   ](https://arxiv.org/abs/1409.3215)\n","- [Learning Phrase Representations using\n","    RNN Encoder-Decoder for Statistical Machine Translation\n","    ](https://arxiv.org/abs/1406.1078)\n","'''\n","#モジュールのインポート\n","from __future__ import print_function\n","\n","from keras.models import Model\n","from keras.layers import Input, LSTM, Dense\n","import numpy as np\n","\n","batch_size = 64  # Batch size for training.\n","epochs = 100  # Number of epochs to train for.\n","latent_dim = 256  # Latent dimensionality of the encoding space.\n","num_samples = 10000  # Number of samples to train on.\n","# Path to the data txt file on disk.\n","data_path = 'C:/Users/圭司/diveintocode-ml/fra-eng/fra.txt'\n","\n","# Vectorize the data.\n","input_texts = []\n","target_texts = []\n","input_characters = set()\n","target_characters = set()\n","# データの読み込み\n","with open(data_path, 'r', encoding='utf-8') as f:\n","    lines = f.read().split('\\n')\n","\n","# データの分割\n","for line in lines[: min(num_samples, len(lines) - 1)]:\n","    input_text, target_text = line.split('\\t')[:-1]\n","    # We use \"tab\" as the \"start sequence\" character\n","    # for the targets, and \"\\n\" as \"end sequence\" character.\n","    target_text = '\\t' + target_text + '\\n'\n","    input_texts.append(input_text)\n","    target_texts.append(target_text)\n","    for char in input_text:\n","        if char not in input_characters:\n","            input_characters.add(char)\n","    for char in target_text:\n","        if char not in target_characters:\n","            target_characters.add(char)\n","\n","input_characters = sorted(list(input_characters))\n","target_characters = sorted(list(target_characters))\n","num_encoder_tokens = len(input_characters)\n","num_decoder_tokens = len(target_characters)\n","max_encoder_seq_length = max([len(txt) for txt in input_texts])\n","max_decoder_seq_length = max([len(txt) for txt in target_texts])\n","\n","print('Number of samples:', len(input_texts))\n","print('Number of unique input tokens:', num_encoder_tokens)\n","print('Number of unique output tokens:', num_decoder_tokens)\n","print('Max sequence length for inputs:', max_encoder_seq_length)\n","print('Max sequence length for outputs:', max_decoder_seq_length)\n","\n","input_token_index = dict(\n","    [(char, i) for i, char in enumerate(input_characters)])\n","target_token_index = dict(\n","    [(char, i) for i, char in enumerate(target_characters)])\n","\n","encoder_input_data = np.zeros(\n","    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n","    dtype='float32')\n","decoder_input_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n","    dtype='float32')\n","decoder_target_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n","    dtype='float32')\n","\n","for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","    for t, char in enumerate(input_text):\n","        encoder_input_data[i, t, input_token_index[char]] = 1.\n","    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n","    for t, char in enumerate(target_text):\n","        # decoder_target_data is ahead of decoder_input_data by one timestep\n","        decoder_input_data[i, t, target_token_index[char]] = 1.\n","        if t > 0:\n","            # decoder_target_data will be ahead by one timestep\n","            # and will not include the start character.\n","            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n","    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n","    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n","# Define an input sequence and process it.\n","encoder_inputs = Input(shape=(None, num_encoder_tokens))\n","encoder = LSTM(latent_dim, return_state=True)\n","encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n","# We discard `encoder_outputs` and only keep the states.\n","encoder_states = [state_h, state_c]\n","\n","# Set up the decoder, using `encoder_states` as initial state.\n","decoder_inputs = Input(shape=(None, num_decoder_tokens))\n","# We set up our decoder to return full output sequences,\n","# and to return internal states as well. We don't use the\n","# return states in the training model, but we will use them in inference.\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n","                                     initial_state=encoder_states)\n","decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Define the model that will turn\n","# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","# Run training\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          validation_split=0.2)\n","# Save model\n","model.save('s2s.h5')\n","\n","# Next: inference mode (sampling).\n","# Here's the drill:\n","# 1) encode input and retrieve initial decoder state\n","# 2) run one step of decoder with this initial state\n","# and a \"start of sequence\" token as target.\n","# Output will be the next target token\n","# 3) Repeat with the current target token and current states\n","\n","# Define sampling models\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","decoder_state_input_h = Input(shape=(latent_dim,))\n","decoder_state_input_c = Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_outputs, state_h, state_c = decoder_lstm(\n","    decoder_inputs, initial_state=decoder_states_inputs)\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs] + decoder_states)\n","\n","# Reverse-lookup token index to decode sequences back to\n","# something readable.\n","reverse_input_char_index = dict(\n","    (i, char) for char, i in input_token_index.items())\n","reverse_target_char_index = dict(\n","    (i, char) for char, i in target_token_index.items())\n","\n","\n","def decode_sequence(input_seq):\n","    # Encode the input as state vectors.\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    # Populate the first character of target sequence with the start character.\n","    target_seq[0, 0, target_token_index['\\t']] = 1.\n","\n","    # Sampling loop for a batch of sequences\n","    # (to simplify, here we assume a batch of size 1).\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict(\n","            [target_seq] + states_value)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = reverse_target_char_index[sampled_token_index]\n","        decoded_sentence += sampled_char\n","\n","        # Exit condition: either hit max length\n","        # or find stop character.\n","        if (sampled_char == '\\n' or\n","           len(decoded_sentence) > max_decoder_seq_length):\n","            stop_condition = True\n","\n","        # Update the target sequence (of length 1).\n","        target_seq = np.zeros((1, 1, num_decoder_tokens))\n","        target_seq[0, 0, sampled_token_index] = 1.\n","\n","        # Update states\n","        states_value = [h, c]\n","\n","    return decoded_sentence\n","\n","\n","for seq_index in range(100):\n","    # Take one sequence (part of the training set)\n","    # for trying out decoding.\n","    input_seq = encoder_input_data[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print('-')\n","    print('Input sentence:', input_texts[seq_index])\n","    print('Decoded sentence:', decoded_sentence)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of samples: 10000\n","Number of unique input tokens: 70\n","Number of unique output tokens: 93\n","Max sequence length for inputs: 16\n","Max sequence length for outputs: 59\n","Train on 8000 samples, validate on 2000 samples\n","Epoch 1/100\n","8000/8000 [==============================] - 36s 5ms/step - loss: 1.2177 - acc: 0.7219 - val_loss: 1.0753 - val_acc: 0.6999\n","Epoch 2/100\n","8000/8000 [==============================] - 35s 4ms/step - loss: 0.8735 - acc: 0.7626 - val_loss: 0.8567 - val_acc: 0.7646\n","Epoch 3/100\n","8000/8000 [==============================] - 34s 4ms/step - loss: 0.6891 - acc: 0.8060 - val_loss: 0.7167 - val_acc: 0.7948\n","Epoch 4/100\n","8000/8000 [==============================] - 35s 4ms/step - loss: 0.5940 - acc: 0.8270 - val_loss: 0.6622 - val_acc: 0.8044\n","Epoch 5/100\n","8000/8000 [==============================] - 35s 4ms/step - loss: 0.5441 - acc: 0.8407 - val_loss: 0.6083 - val_acc: 0.8208\n","Epoch 6/100\n","8000/8000 [==============================] - 35s 4ms/step - loss: 0.5070 - acc: 0.8506 - val_loss: 0.5898 - val_acc: 0.8262\n","Epoch 7/100\n","8000/8000 [==============================] - 36s 4ms/step - loss: 0.4775 - acc: 0.8587 - val_loss: 0.5594 - val_acc: 0.8343\n","Epoch 8/100\n","8000/8000 [==============================] - 35s 4ms/step - loss: 0.4526 - acc: 0.8656 - val_loss: 0.5313 - val_acc: 0.8408\n","Epoch 9/100\n","8000/8000 [==============================] - 36s 4ms/step - loss: 0.4309 - acc: 0.8713 - val_loss: 0.5187 - val_acc: 0.8467\n","Epoch 10/100\n","8000/8000 [==============================] - 39s 5ms/step - loss: 0.4117 - acc: 0.8771 - val_loss: 0.5019 - val_acc: 0.8508\n","Epoch 11/100\n","8000/8000 [==============================] - 36s 5ms/step - loss: 0.3939 - acc: 0.8822 - val_loss: 0.4921 - val_acc: 0.8533\n","Epoch 12/100\n","8000/8000 [==============================] - 35s 4ms/step - loss: 0.3782 - acc: 0.8867 - val_loss: 0.4800 - val_acc: 0.8558\n","Epoch 13/100\n","8000/8000 [==============================] - 36s 4ms/step - loss: 0.3631 - acc: 0.8910 - val_loss: 0.4701 - val_acc: 0.8603\n","Epoch 14/100\n","8000/8000 [==============================] - 35s 4ms/step - loss: 0.3495 - acc: 0.8946 - val_loss: 0.4631 - val_acc: 0.8626\n","Epoch 15/100\n","8000/8000 [==============================] - 35s 4ms/step - loss: 0.3365 - acc: 0.8986 - val_loss: 0.4568 - val_acc: 0.8638\n","Epoch 16/100\n","8000/8000 [==============================] - 35s 4ms/step - loss: 0.3239 - acc: 0.9023 - val_loss: 0.4565 - val_acc: 0.8653\n","Epoch 17/100\n","8000/8000 [==============================] - 36s 4ms/step - loss: 0.3121 - acc: 0.9061 - val_loss: 0.4504 - val_acc: 0.8673\n","Epoch 18/100\n","8000/8000 [==============================] - 36s 5ms/step - loss: 0.3010 - acc: 0.9091 - val_loss: 0.4484 - val_acc: 0.8681\n","Epoch 19/100\n","8000/8000 [==============================] - 36s 5ms/step - loss: 0.2903 - acc: 0.9122 - val_loss: 0.4480 - val_acc: 0.8686\n","Epoch 20/100\n","8000/8000 [==============================] - 35s 4ms/step - loss: 0.2801 - acc: 0.9154 - val_loss: 0.4444 - val_acc: 0.8700\n","Epoch 21/100\n","8000/8000 [==============================] - 35s 4ms/step - loss: 0.2706 - acc: 0.9182 - val_loss: 0.4419 - val_acc: 0.8712\n","Epoch 22/100\n","8000/8000 [==============================] - 36s 4ms/step - loss: 0.2615 - acc: 0.9209 - val_loss: 0.4454 - val_acc: 0.8713\n","Epoch 23/100\n","8000/8000 [==============================] - 36s 5ms/step - loss: 0.2528 - acc: 0.9235 - val_loss: 0.4434 - val_acc: 0.8723\n","Epoch 24/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.2444 - acc: 0.9259 - val_loss: 0.4473 - val_acc: 0.8715\n","Epoch 25/100\n","8000/8000 [==============================] - 36s 5ms/step - loss: 0.2361 - acc: 0.9285 - val_loss: 0.4507 - val_acc: 0.8722\n","Epoch 26/100\n","8000/8000 [==============================] - 38s 5ms/step - loss: 0.2289 - acc: 0.9300 - val_loss: 0.4489 - val_acc: 0.8737\n","Epoch 27/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.2215 - acc: 0.9326 - val_loss: 0.4505 - val_acc: 0.8722\n","Epoch 28/100\n","8000/8000 [==============================] - 36s 4ms/step - loss: 0.2146 - acc: 0.9345 - val_loss: 0.4525 - val_acc: 0.8739\n","Epoch 29/100\n","8000/8000 [==============================] - 36s 4ms/step - loss: 0.2075 - acc: 0.9367 - val_loss: 0.4540 - val_acc: 0.8738\n","Epoch 30/100\n","8000/8000 [==============================] - 36s 4ms/step - loss: 0.2013 - acc: 0.9388 - val_loss: 0.4589 - val_acc: 0.8743\n","Epoch 31/100\n","8000/8000 [==============================] - 36s 4ms/step - loss: 0.1953 - acc: 0.9403 - val_loss: 0.4617 - val_acc: 0.8744\n","Epoch 32/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.1893 - acc: 0.9421 - val_loss: 0.4678 - val_acc: 0.8735\n","Epoch 33/100\n","8000/8000 [==============================] - 36s 5ms/step - loss: 0.1834 - acc: 0.9443 - val_loss: 0.4720 - val_acc: 0.8741\n","Epoch 34/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.1782 - acc: 0.9454 - val_loss: 0.4772 - val_acc: 0.8737\n","Epoch 35/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.1730 - acc: 0.9471 - val_loss: 0.4805 - val_acc: 0.8734\n","Epoch 36/100\n","8000/8000 [==============================] - 36s 5ms/step - loss: 0.1680 - acc: 0.9487 - val_loss: 0.4793 - val_acc: 0.8743\n","Epoch 37/100\n","8000/8000 [==============================] - 36s 5ms/step - loss: 0.1632 - acc: 0.9499 - val_loss: 0.4879 - val_acc: 0.8736\n","Epoch 38/100\n","8000/8000 [==============================] - 36s 5ms/step - loss: 0.1587 - acc: 0.9512 - val_loss: 0.4913 - val_acc: 0.8736\n","Epoch 39/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.1541 - acc: 0.9526 - val_loss: 0.4942 - val_acc: 0.8743\n","Epoch 40/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.1495 - acc: 0.9542 - val_loss: 0.4977 - val_acc: 0.8741\n","Epoch 41/100\n","8000/8000 [==============================] - 36s 4ms/step - loss: 0.1457 - acc: 0.9551 - val_loss: 0.4981 - val_acc: 0.8756\n","Epoch 42/100\n","8000/8000 [==============================] - 36s 5ms/step - loss: 0.1415 - acc: 0.9567 - val_loss: 0.5115 - val_acc: 0.8736\n","Epoch 43/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.1382 - acc: 0.9575 - val_loss: 0.5156 - val_acc: 0.8736\n","Epoch 44/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.1345 - acc: 0.9585 - val_loss: 0.5204 - val_acc: 0.8734\n","Epoch 45/100\n","8000/8000 [==============================] - 39s 5ms/step - loss: 0.1310 - acc: 0.9596 - val_loss: 0.5206 - val_acc: 0.8741\n","Epoch 46/100\n","8000/8000 [==============================] - 39s 5ms/step - loss: 0.1279 - acc: 0.9604 - val_loss: 0.5292 - val_acc: 0.8740\n","Epoch 47/100\n","8000/8000 [==============================] - 39s 5ms/step - loss: 0.1244 - acc: 0.9615 - val_loss: 0.5324 - val_acc: 0.8739\n","Epoch 48/100\n","8000/8000 [==============================] - 41s 5ms/step - loss: 0.1215 - acc: 0.9625 - val_loss: 0.5354 - val_acc: 0.8738\n","Epoch 49/100\n","8000/8000 [==============================] - 43s 5ms/step - loss: 0.1184 - acc: 0.9631 - val_loss: 0.5427 - val_acc: 0.8736\n","Epoch 50/100\n","8000/8000 [==============================] - 40s 5ms/step - loss: 0.1154 - acc: 0.9641 - val_loss: 0.5456 - val_acc: 0.8737\n","Epoch 51/100\n","8000/8000 [==============================] - 39s 5ms/step - loss: 0.1123 - acc: 0.9653 - val_loss: 0.5585 - val_acc: 0.8729\n","Epoch 52/100\n","8000/8000 [==============================] - 36s 5ms/step - loss: 0.1099 - acc: 0.9659 - val_loss: 0.5588 - val_acc: 0.8726\n","Epoch 53/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.1071 - acc: 0.9665 - val_loss: 0.5648 - val_acc: 0.8724\n","Epoch 54/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.1047 - acc: 0.9673 - val_loss: 0.5652 - val_acc: 0.8739\n","Epoch 55/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.1023 - acc: 0.9680 - val_loss: 0.5712 - val_acc: 0.8737\n","Epoch 56/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.0999 - acc: 0.9688 - val_loss: 0.5798 - val_acc: 0.8721\n","Epoch 57/100\n","8000/8000 [==============================] - 36s 4ms/step - loss: 0.0977 - acc: 0.9694 - val_loss: 0.5782 - val_acc: 0.8737\n","Epoch 58/100\n","8000/8000 [==============================] - 36s 4ms/step - loss: 0.0952 - acc: 0.9699 - val_loss: 0.5867 - val_acc: 0.8728\n","Epoch 59/100\n"],"name":"stdout"},{"output_type":"stream","text":["8000/8000 [==============================] - 36s 4ms/step - loss: 0.0934 - acc: 0.9704 - val_loss: 0.5951 - val_acc: 0.8717\n","Epoch 60/100\n","8000/8000 [==============================] - 36s 4ms/step - loss: 0.0915 - acc: 0.9712 - val_loss: 0.5884 - val_acc: 0.8729\n","Epoch 61/100\n","8000/8000 [==============================] - 35s 4ms/step - loss: 0.0890 - acc: 0.9719 - val_loss: 0.5976 - val_acc: 0.8736\n","Epoch 62/100\n","8000/8000 [==============================] - 35s 4ms/step - loss: 0.0873 - acc: 0.9723 - val_loss: 0.6035 - val_acc: 0.8727\n","Epoch 63/100\n","8000/8000 [==============================] - 36s 4ms/step - loss: 0.0859 - acc: 0.9726 - val_loss: 0.6082 - val_acc: 0.8728\n","Epoch 64/100\n","8000/8000 [==============================] - 36s 4ms/step - loss: 0.0836 - acc: 0.9735 - val_loss: 0.6117 - val_acc: 0.8730\n","Epoch 65/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.0816 - acc: 0.9741 - val_loss: 0.6198 - val_acc: 0.8718\n","Epoch 66/100\n","8000/8000 [==============================] - 36s 5ms/step - loss: 0.0804 - acc: 0.9744 - val_loss: 0.6180 - val_acc: 0.8733\n","Epoch 67/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.0785 - acc: 0.9748 - val_loss: 0.6284 - val_acc: 0.8725\n","Epoch 68/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.0768 - acc: 0.9753 - val_loss: 0.6265 - val_acc: 0.8727\n","Epoch 69/100\n","8000/8000 [==============================] - 36s 4ms/step - loss: 0.0755 - acc: 0.9757 - val_loss: 0.6331 - val_acc: 0.8714\n","Epoch 70/100\n","8000/8000 [==============================] - 36s 4ms/step - loss: 0.0739 - acc: 0.9762 - val_loss: 0.6423 - val_acc: 0.8714\n","Epoch 71/100\n","8000/8000 [==============================] - 36s 4ms/step - loss: 0.0726 - acc: 0.9766 - val_loss: 0.6458 - val_acc: 0.8716\n","Epoch 72/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.0709 - acc: 0.9769 - val_loss: 0.6454 - val_acc: 0.8724\n","Epoch 73/100\n","8000/8000 [==============================] - 36s 5ms/step - loss: 0.0699 - acc: 0.9772 - val_loss: 0.6510 - val_acc: 0.8728\n","Epoch 74/100\n","8000/8000 [==============================] - 36s 5ms/step - loss: 0.0682 - acc: 0.9779 - val_loss: 0.6531 - val_acc: 0.8720\n","Epoch 75/100\n","8000/8000 [==============================] - 36s 5ms/step - loss: 0.0669 - acc: 0.9783 - val_loss: 0.6598 - val_acc: 0.8711\n","Epoch 76/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.0660 - acc: 0.9785 - val_loss: 0.6598 - val_acc: 0.8718\n","Epoch 77/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.0646 - acc: 0.9786 - val_loss: 0.6623 - val_acc: 0.8720\n","Epoch 78/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.0637 - acc: 0.9791 - val_loss: 0.6647 - val_acc: 0.8716\n","Epoch 79/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.0624 - acc: 0.9792 - val_loss: 0.6738 - val_acc: 0.8714\n","Epoch 80/100\n","8000/8000 [==============================] - 38s 5ms/step - loss: 0.0611 - acc: 0.9797 - val_loss: 0.6850 - val_acc: 0.8705\n","Epoch 81/100\n","8000/8000 [==============================] - 36s 5ms/step - loss: 0.0600 - acc: 0.9802 - val_loss: 0.6792 - val_acc: 0.8716\n","Epoch 82/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.0588 - acc: 0.9804 - val_loss: 0.6828 - val_acc: 0.8709\n","Epoch 83/100\n","8000/8000 [==============================] - 39s 5ms/step - loss: 0.0579 - acc: 0.9807 - val_loss: 0.6858 - val_acc: 0.8721\n","Epoch 84/100\n","8000/8000 [==============================] - 39s 5ms/step - loss: 0.0570 - acc: 0.9810 - val_loss: 0.6937 - val_acc: 0.8712\n","Epoch 85/100\n","8000/8000 [==============================] - 38s 5ms/step - loss: 0.0560 - acc: 0.9812 - val_loss: 0.6980 - val_acc: 0.8710\n","Epoch 86/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.0550 - acc: 0.9815 - val_loss: 0.6986 - val_acc: 0.8710\n","Epoch 87/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.0544 - acc: 0.9817 - val_loss: 0.6980 - val_acc: 0.8712\n","Epoch 88/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.0532 - acc: 0.9820 - val_loss: 0.7041 - val_acc: 0.8714\n","Epoch 89/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.0524 - acc: 0.9823 - val_loss: 0.7143 - val_acc: 0.8702\n","Epoch 90/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.0517 - acc: 0.9823 - val_loss: 0.7130 - val_acc: 0.8716\n","Epoch 91/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.0507 - acc: 0.9826 - val_loss: 0.7108 - val_acc: 0.8713\n","Epoch 92/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.0497 - acc: 0.9830 - val_loss: 0.7224 - val_acc: 0.8707\n","Epoch 93/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.0493 - acc: 0.9831 - val_loss: 0.7263 - val_acc: 0.8715\n","Epoch 94/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.0489 - acc: 0.9833 - val_loss: 0.7234 - val_acc: 0.8706\n","Epoch 95/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.0478 - acc: 0.9836 - val_loss: 0.7263 - val_acc: 0.8708\n","Epoch 96/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.0469 - acc: 0.9839 - val_loss: 0.7324 - val_acc: 0.8706\n","Epoch 97/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.0465 - acc: 0.9838 - val_loss: 0.7340 - val_acc: 0.8704\n","Epoch 98/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.0459 - acc: 0.9840 - val_loss: 0.7387 - val_acc: 0.8708\n","Epoch 99/100\n","8000/8000 [==============================] - 37s 5ms/step - loss: 0.0450 - acc: 0.9844 - val_loss: 0.7388 - val_acc: 0.8703\n","Epoch 100/100\n","8000/8000 [==============================] - 38s 5ms/step - loss: 0.0447 - acc: 0.9844 - val_loss: 0.7452 - val_acc: 0.8704\n"],"name":"stdout"},{"output_type":"stream","text":["C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer lstm_6 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_5/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_5/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n","  '. They will not be included '\n"],"name":"stderr"},{"output_type":"stream","text":["-\n","Input sentence: Go.\n","Decoded sentence: Va !\n","\n","-\n","Input sentence: Hi.\n","Decoded sentence: Salut.\n","\n","-\n","Input sentence: Hi.\n","Decoded sentence: Salut.\n","\n","-\n","Input sentence: Run!\n","Decoded sentence: Cours !\n","\n","-\n","Input sentence: Run!\n","Decoded sentence: Cours !\n","\n","-\n","Input sentence: Who?\n","Decoded sentence: Qui ?\n","\n","-\n","Input sentence: Wow!\n","Decoded sentence: Ça alors !\n","\n","-\n","Input sentence: Fire!\n","Decoded sentence: Trèie Tom.\n","\n","-\n","Input sentence: Help!\n","Decoded sentence: À l'aide !\n","\n","-\n","Input sentence: Jump.\n","Decoded sentence: Saute.\n","\n","-\n","Input sentence: Stop!\n","Decoded sentence: Stop !\n","\n","-\n","Input sentence: Stop!\n","Decoded sentence: Stop !\n","\n","-\n","Input sentence: Stop!\n","Decoded sentence: Stop !\n","\n","-\n","Input sentence: Wait!\n","Decoded sentence: Attendez !\n","\n","-\n","Input sentence: Wait!\n","Decoded sentence: Attendez !\n","\n","-\n","Input sentence: Go on.\n","Decoded sentence: Poursuis.\n","\n","-\n","Input sentence: Go on.\n","Decoded sentence: Poursuis.\n","\n","-\n","Input sentence: Go on.\n","Decoded sentence: Poursuis.\n","\n","-\n","Input sentence: Hello!\n","Decoded sentence: Salut !\n","\n","-\n","Input sentence: Hello!\n","Decoded sentence: Salut !\n","\n","-\n","Input sentence: I see.\n","Decoded sentence: Je comprends.\n","\n","-\n","Input sentence: I try.\n","Decoded sentence: J'essaye.\n","\n","-\n","Input sentence: I won!\n","Decoded sentence: J'ai gagné !\n","\n","-\n","Input sentence: I won!\n","Decoded sentence: J'ai gagné !\n","\n","-\n","Input sentence: I won.\n","Decoded sentence: J’ai gagné.\n","\n","-\n","Input sentence: Oh no!\n","Decoded sentence: Oh non !\n","\n","-\n","Input sentence: Attack!\n","Decoded sentence: Attaque !\n","\n","-\n","Input sentence: Attack!\n","Decoded sentence: Attaque !\n","\n","-\n","Input sentence: Cheers!\n","Decoded sentence: Santé !\n","\n","-\n","Input sentence: Cheers!\n","Decoded sentence: Santé !\n","\n","-\n","Input sentence: Cheers!\n","Decoded sentence: Santé !\n","\n","-\n","Input sentence: Cheers!\n","Decoded sentence: Santé !\n","\n","-\n","Input sentence: Get up.\n","Decoded sentence: Descendez !\n","\n","-\n","Input sentence: Go now.\n","Decoded sentence: Allez-y maintenant.\n","\n","-\n","Input sentence: Go now.\n","Decoded sentence: Allez-y maintenant.\n","\n","-\n","Input sentence: Go now.\n","Decoded sentence: Allez-y maintenant.\n","\n","-\n","Input sentence: Got it!\n","Decoded sentence: Compris !\n","\n","-\n","Input sentence: Got it!\n","Decoded sentence: Compris !\n","\n","-\n","Input sentence: Got it?\n","Decoded sentence: Pigé ?\n","\n","-\n","Input sentence: Got it?\n","Decoded sentence: Pigé ?\n","\n","-\n","Input sentence: Got it?\n","Decoded sentence: Pigé ?\n","\n","-\n","Input sentence: Hop in.\n","Decoded sentence: Montez.\n","\n","-\n","Input sentence: Hop in.\n","Decoded sentence: Montez.\n","\n","-\n","Input sentence: Hug me.\n","Decoded sentence: Serre-moi dans tes bras !\n","\n","-\n","Input sentence: Hug me.\n","Decoded sentence: Serre-moi dans tes bras !\n","\n","-\n","Input sentence: I fell.\n","Decoded sentence: Je suis tombée.\n","\n","-\n","Input sentence: I fell.\n","Decoded sentence: Je suis tombée.\n","\n","-\n","Input sentence: I know.\n","Decoded sentence: Je sais.\n","\n","-\n","Input sentence: I left.\n","Decoded sentence: Je suis parti.\n","\n","-\n","Input sentence: I left.\n","Decoded sentence: Je suis parti.\n","\n","-\n","Input sentence: I lost.\n","Decoded sentence: J'ai perdu.\n","\n","-\n","Input sentence: I paid.\n","Decoded sentence: J'essayai.\n","\n","-\n","Input sentence: I'm 19.\n","Decoded sentence: J'ai 19 ans.\n","\n","-\n","Input sentence: I'm OK.\n","Decoded sentence: Je vais bien.\n","\n","-\n","Input sentence: I'm OK.\n","Decoded sentence: Je vais bien.\n","\n","-\n","Input sentence: Listen.\n","Decoded sentence: Écoutez !\n","\n","-\n","Input sentence: No way!\n","Decoded sentence: En aucun cas.\n","\n","-\n","Input sentence: No way!\n","Decoded sentence: En aucun cas.\n","\n","-\n","Input sentence: No way!\n","Decoded sentence: En aucun cas.\n","\n","-\n","Input sentence: No way!\n","Decoded sentence: En aucun cas.\n","\n","-\n","Input sentence: No way!\n","Decoded sentence: En aucun cas.\n","\n","-\n","Input sentence: No way!\n","Decoded sentence: En aucun cas.\n","\n","-\n","Input sentence: No way!\n","Decoded sentence: En aucun cas.\n","\n","-\n","Input sentence: No way!\n","Decoded sentence: En aucun cas.\n","\n","-\n","Input sentence: No way!\n","Decoded sentence: En aucun cas.\n","\n","-\n","Input sentence: Really?\n","Decoded sentence: Vrai ?\n","\n","-\n","Input sentence: Really?\n","Decoded sentence: Vrai ?\n","\n","-\n","Input sentence: Really?\n","Decoded sentence: Vrai ?\n","\n","-\n","Input sentence: Thanks.\n","Decoded sentence: Merci !\n","\n","-\n","Input sentence: We try.\n","Decoded sentence: On essaye.\n","\n","-\n","Input sentence: We won.\n","Decoded sentence: Nous l'avons emporté.\n","\n","-\n","Input sentence: We won.\n","Decoded sentence: Nous l'avons emporté.\n","\n","-\n","Input sentence: We won.\n","Decoded sentence: Nous l'avons emporté.\n","\n","-\n","Input sentence: We won.\n","Decoded sentence: Nous l'avons emporté.\n","\n","-\n","Input sentence: Ask Tom.\n","Decoded sentence: Demande à Tom.\n","\n","-\n","Input sentence: Awesome!\n","Decoded sentence: Fantastique !\n","\n","-\n","Input sentence: Be calm.\n","Decoded sentence: Soyez calmes !\n","\n","-\n","Input sentence: Be calm.\n","Decoded sentence: Soyez calmes !\n","\n","-\n","Input sentence: Be calm.\n","Decoded sentence: Soyez calmes !\n","\n","-\n","Input sentence: Be cool.\n","Decoded sentence: Sois détendu !\n","\n","-\n","Input sentence: Be fair.\n","Decoded sentence: Soyez équitables !\n","\n","-\n","Input sentence: Be fair.\n","Decoded sentence: Soyez équitables !\n","\n","-\n","Input sentence: Be fair.\n","Decoded sentence: Soyez équitables !\n","\n","-\n","Input sentence: Be fair.\n","Decoded sentence: Soyez équitables !\n","\n","-\n","Input sentence: Be fair.\n","Decoded sentence: Soyez équitables !\n","\n","-\n","Input sentence: Be fair.\n","Decoded sentence: Soyez équitables !\n","\n","-\n","Input sentence: Be kind.\n","Decoded sentence: Sois gentil.\n","\n","-\n","Input sentence: Be nice.\n","Decoded sentence: Soyez gentil !\n","\n","-\n","Input sentence: Be nice.\n","Decoded sentence: Soyez gentil !\n","\n","-\n","Input sentence: Be nice.\n","Decoded sentence: Soyez gentil !\n","\n","-\n","Input sentence: Be nice.\n","Decoded sentence: Soyez gentil !\n","\n","-\n","Input sentence: Be nice.\n","Decoded sentence: Soyez gentil !\n","\n","-\n","Input sentence: Be nice.\n","Decoded sentence: Soyez gentil !\n","\n","-\n","Input sentence: Beat it.\n","Decoded sentence: Dégage !\n","\n","-\n","Input sentence: Call me.\n","Decoded sentence: Appelle-moi !\n","\n","-\n","Input sentence: Call me.\n","Decoded sentence: Appelle-moi !\n","\n","-\n","Input sentence: Call us.\n","Decoded sentence: Appelle-nous !\n","\n","-\n","Input sentence: Call us.\n","Decoded sentence: Appelle-nous !\n","\n","-\n","Input sentence: Come in.\n","Decoded sentence: Entre !\n","\n","-\n","Input sentence: Come in.\n","Decoded sentence: Entre !\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yW__ZyWJcuPu","colab_type":"text"},"source":["## 3.イメージキャプショニング\n","他の活用例としてイメージキャプショニングがあります。画像に対する説明の文章を推定するタスクです。これは画像を入力し、系列データを出力する Image to Sequence の手法によって行えます。\n","\n","[pytorch-tutorial/tutorials/03-advanced/image_captioning at master · yunjey/pytorch-tutorial](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/image_captioning)\n","\n","イメージキャプショニングは学習に多くの時間がかかるため、ここでは学習済みの重みが公開されている実装を動かすことにします。Kerasには平易に扱える実装が公開されていないため、今回はPyTorchによる実装を扱います。"]},{"cell_type":"markdown","metadata":{"id":"gxd6iCXxcuPv","colab_type":"text"},"source":["### 【問題2】イメージキャプショニングの学習済みモデルの実行\n","上記実装において 5. Test the model の項目を実行してください。また、自身で用意した画像に対しても文章を生成してください。これらに対してどういった文章が出力されたかを記録して提出してください。\n","\n","データセットからの学習は行わず、学習済みの重みをダウンロードして利用します。\n","\n","注意点として、デフォルトで設定されている重みのファイル名と、ダウンロードできる重みのファイル名は異なっています。ここは書き換える必要があります。"]},{"cell_type":"code","metadata":{"id":"5smitB8Bc6CO","colab_type":"code","outputId":"56eb09c1-0782-4978-8ad9-319e9751ec48","executionInfo":{"status":"ok","timestamp":1572418683498,"user_tz":-540,"elapsed":3666,"user":{"displayName":"keiji kakuta","photoUrl":"","userId":"11779816064173035676"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["!git clone https://github.com/yunjey/pytorch-tutorial.git"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cloning into 'pytorch-tutorial'...\n","remote: Enumerating objects: 816, done.\u001b[K\n","remote: Total 816 (delta 0), reused 0 (delta 0), pack-reused 816\u001b[K\n","Receiving objects: 100% (816/816), 12.78 MiB | 42.63 MiB/s, done.\n","Resolving deltas: 100% (432/432), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P9uuTM18dn4c","colab_type":"code","outputId":"380101e0-9294-4747-b7b5-cd154fd95806","executionInfo":{"status":"ok","timestamp":1572419752612,"user_tz":-540,"elapsed":512,"user":{"displayName":"keiji kakuta","photoUrl":"","userId":"11779816064173035676"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd \"/content/pytorch-tutorial/tutorials/03-advanced/image_captioning\""],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content/pytorch-tutorial/tutorials/03-advanced/image_captioning\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IM9tCH__neL5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"9614f63c-7abb-4ddf-8730-fa42a603a118","executionInfo":{"status":"ok","timestamp":1572421376973,"user_tz":-540,"elapsed":493,"user":{"displayName":"keiji kakuta","photoUrl":"","userId":"11779816064173035676"}}},"source":["%cd /content/drive/My Drive/pytorch-tutorial/tutorials/03-advanced/image_captioning"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/pytorch-tutorial/tutorials/03-advanced/image_captioning\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"79HDygMYv1wZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"c013cbc0-18e9-4996-8509-97025dd9d935","executionInfo":{"status":"ok","timestamp":1572423575829,"user_tz":-540,"elapsed":5880,"user":{"displayName":"keiji kakuta","photoUrl":"","userId":"11779816064173035676"}}},"source":["!unzip \"/content/drive/My Drive/pytorch-tutorial/tutorials/03-advanced/image_captioning/models/pretrained_model.zip\""],"execution_count":7,"outputs":[{"output_type":"stream","text":["Archive:  /content/drive/My Drive/pytorch-tutorial/tutorials/03-advanced/image_captioning/models/pretrained_model.zip\n","  inflating: encoder-5-3000.pkl      \n","  inflating: decoder-5-3000.pkl      \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-F7XC-iChjb-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ac2db460-63da-4d81-db61-bd7a76a41d67","executionInfo":{"status":"ok","timestamp":1572423695998,"user_tz":-540,"elapsed":9132,"user":{"displayName":"keiji kakuta","photoUrl":"","userId":"11779816064173035676"}}},"source":["!python sample.py --image='/content/drive/My Drive/pytorch-tutorial/cat_05.jpg' "],"execution_count":11,"outputs":[{"output_type":"stream","text":["<start> a cat is laying on a bed with a remote . <end>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L8kjDJ7McuP2","colab_type":"text"},"source":["### 【問題3】Kerasで動かしたい場合はどうするかを調査\n","PyTorchによる実装を動かしましたが、何らかの理由からKerasで動かしたい状況が考えられます。どういった手順を踏むことになるか調査し、できるだけ詳しく説明してください。\n","\n","特に今回はPyTorchのための学習済みの重みをKerasで使えるようにしたいので、その点については必ず触れてください。"]},{"cell_type":"markdown","metadata":{"id":"9-nVEI0fcuP3","colab_type":"text"},"source":["### 【解答】\n","- PyTorchのモデルをONNX形式のモデルに変換してからKerasのモデルに変換する"]},{"cell_type":"markdown","metadata":{"id":"T-7Pnr9zcuP4","colab_type":"text"},"source":["### 【問題4】（アドバンス課題）コードリーディングと書き換え\n","モデル部分はmodel.pyに書かれていますが、Kerasではこのモデルがどのように記述できるかを考え、コーディングしてください。その際機械翻訳のサンプルコードが参考になります。"]},{"cell_type":"markdown","metadata":{"id":"bTjGuHpJcuP5","colab_type":"text"},"source":["### 【問題5】（アドバンス課題）発展的調査\n","《他の言語の翻訳を行う場合は？》\n","\n","問題1の実装を使い日本語と英語の翻訳を行いたい場合はどのような手順を踏むか考えてみましょう。\n","\n","《機械翻訳の発展的手法にはどのようなものがある？》\n","\n","機械翻訳のための発展的手法にはどういったものがあるか調査してみましょう。\n","\n","《文章から画像生成するには？》\n","\n","イメージキャプショニングとは逆に文章から画像を生成する手法もあります。どういったものがあるか調査してみましょう。"]},{"cell_type":"code","metadata":{"id":"2WajYBFScuP6","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}