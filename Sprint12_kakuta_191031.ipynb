{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint12 深層学習スクラッチ 畳み込みニューラルネットワーク2\n",
    "\n",
    "## 1.このSprintについて\n",
    "\n",
    "### Sprintの目的\n",
    "- スクラッチを通してCNNの基礎を理解する\n",
    "\n",
    "### どのように学ぶか\n",
    "スクラッチで2次元用畳み込みニューラルネットワークを実装した後、学習と検証を行なっていきます。\n",
    "\n",
    "## 2.2次元の畳み込みニューラルネットワークスクラッチ\n",
    "2次元に対応した畳み込みニューラルネットワーク（CNN）のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "プーリング層なども作成することで、CNNの基本形を完成させます。クラスの名前はScratch2dCNNClassifierとしてください。\n",
    "\n",
    "### データセットの用意\n",
    "引き続きMNISTデータセットを使用します。2次元畳み込み層へは、28×28の状態で入力します。\n",
    "\n",
    "今回は白黒画像ですからチャンネルは1つしかありませんが、チャンネル方向の軸は用意しておく必要があります。\n",
    "\n",
    "(n_samples, n_channels, height, width)のNCHWまたは(n_samples, height, width, n_channels)のNHWCどちらかの形にしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "(X_train, t_train), (X_test, t_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "t_train_one_hot = enc.fit_transform(t_train[:, np.newaxis])\n",
    "t_test_one_hot = enc.fit_transform(t_test[:,  np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 1, 28, 28)\n",
    "X_test = X_test.reshape(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d:\n",
    "    \n",
    "    def __init__(self, initializer, out_chanel, in_chanel, height, width, optimizer):\n",
    "        init = initializer\n",
    "        self.w = init.W(out_chanel, in_chanel, height, width)\n",
    "        self.b = init.B(out_chanel)\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.sample_size, self.in_chanel, self.x_height, self.x_width = X.shape\n",
    "        self.out_chanel, self.inchanel, self.w_height, self.w_width = self.w.shape\n",
    "        self.XB = X\n",
    "        \n",
    "        \n",
    "        A  = np.zeros([self.sample_size,  self.out_chanel, self.x_height -2, self.x_width -2])\n",
    "        for n in range(self.sample_size):\n",
    "            for outchan in range(self.out_chanel):\n",
    "                for inchan in range(self.in_chanel):\n",
    "                    for i in range(self.x_height-2):\n",
    "                        for j in range(self.x_width-2):\n",
    "                            sig = 0\n",
    "                            for s in range(self.w_height):\n",
    "                                for t in range(self.w_width):\n",
    "                                    sig += X[n, inchan, i+s, j+t] * self.w[outchan, inchan, s, t]\n",
    "                        A[n, outchan, i, j] += sig + self.b[outchan]\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        n_out_h , n_out_w = N_out(self.x_height, self.x_width, 0, self.w_height, self.w_width, 1)\n",
    "        self.lb = dA.sum(axis=(0, 2, 3))\n",
    "        \n",
    "        self.lw = np.zeros_like(self.w)\n",
    "        for n in range(self.sample_size):\n",
    "            for m in range(self.out_chanel):\n",
    "                for k in range(self.in_chanel):\n",
    "                    for s in range(self.w_height):\n",
    "                        for t in range(self.w_width):\n",
    "                            for i in range(self.w_height-1):\n",
    "                                for j in range(self.w_width-1):\n",
    "                                    self.lw[m, k, s, t] += dA[n, m, i, j] * self.XB[n, k, i+s, j+t]\n",
    "        \n",
    "        \n",
    "        dZ = np.zeros_like(self.XB)\n",
    "        for n in range(self.sample_size):\n",
    "            for m in range(self.out_chanel):\n",
    "                for k in range(self.inchanel):\n",
    "                    for i in range(self.x_height):\n",
    "                        for j in range(self.x_width):\n",
    "                            sig = 0\n",
    "                            for s in range(self.w_height):\n",
    "                                for t in range(self.w_width):\n",
    "                                    if i-s<0 or i-s>n_out_h-1 or j-t < 0 or j-t>n_out_w-1:\n",
    "                                        pass\n",
    "                                    else:\n",
    "                                        sig += dA[n, m, i-s, j-t] * self.w[m, k, s, t]\n",
    "                            dZ[n, k, i, j] += sig\n",
    "        \n",
    "        \n",
    "        \n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】2次元畳み込み層の作成\n",
    "1次元畳み込み層のクラスConv1dを発展させ、2次元畳み込み層のクラスConv2dを作成してください。\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。\n",
    "$$a_{i, j, m} = \\sum_{k=0}^{K-1} \\sum_{s=0}^{F_h -1}\\sum_{t=0}^{F_w -1} x_{(i+s),(j+t),k}w_{s,t,k,m} + b_m$$\n",
    "\n",
    "$a_{i,j,m}$  : 出力される配列のi行j列、mチャンネルの値\n",
    "\n",
    "$i$ : 配列の行方向のインデックス\n",
    "\n",
    "$j$ : 配列の列方向のインデックス\n",
    "\n",
    "$m$ : 出力チャンネルのインデックス\n",
    "\n",
    "$K$ : 入力チャンネル数\n",
    "\n",
    "$F_h,F_w$ : 高さ方向（h）と幅方向（w）のフィルタのサイズ\n",
    "\n",
    "$x_{(i+s),(j+t),k}$ : 入力の配列の(i+s)行(j+t)列、kチャンネルの値\n",
    "\n",
    "$w_{s,t,k,m}$ : 重みの配列のs行t列目。kチャンネルの入力に対して、mチャンネルへ出力する重み\n",
    "\n",
    "$b_m$ : mチャンネルへの出力のバイアス項\n",
    "\n",
    "全てスカラーです。\n",
    "\n",
    "次に更新式です。1次元畳み込み層や全結合層と同じ形です。\n",
    "$$w'_{s, t, k, m} = w_{s, t, k, m} - \\alpha \\frac{\\partial L}{\\partial w_{s, t, k, m}}$$\n",
    "\n",
    "$$b'_m = b_m - \\alpha \\frac{\\partial L}{\\partial b_m}$$\n",
    "\n",
    "$\\alpha$  : 学習率\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_{s, t, k, m}}$ ： $w_{s,t,k,m}$  に関する損失 $L$ の勾配\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b_m}$ : $b_m$  に関する損失$L$ の勾配\n",
    "\n",
    "勾配$\\frac{\\partial L}{\\partial w_{s, t, k, m}}$ や $\\frac{\\partial L}{\\partial b_m}$ を求めるためのバックプロパゲーションの数式が以下である。\n",
    "$$\\frac{\\partial L}{\\partial w_{s, t, k, m}} = \\sum_{i=0}^{N_{out, h}-1} \\sum_{j=0}^{N_{out, w}-1} \\frac{\\partial L}{\\partial a_{i,j,m}} x_{(i+s)(j+k),k}$$\n",
    "$$\\frac{\\partial L}{\\partial b_m} =  \\sum_{i=0}^{N_{out, h}-1} \\sum_{j=0}^{N_{out, w}-1} \\frac{\\partial L}{\\partial a_{i,j,m}}$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial a_{i,j,m}}$ : 勾配の配列のi行j列、mチャンネルの値\n",
    "\n",
    "$N_{out,h},N_{out,w}$  : 高さ方向（h）と幅方向（w）の出力のサイズ\n",
    "\n",
    "前の層に流す誤差の数式は以下です。\n",
    "$$\\frac{\\partial L}{\\partial x_{i,j,k}} = \\sum_{m=0}^{M-1} \\sum_{s=0}^{F_{h-1}} \\sum_{t=0}^{F_{w-1}} \\frac{\\partial L}{\\partial a_{(i-s)(j-t),m}}w_{s, t, k, m}$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_{i,j,k}}$ : 前の層に流す誤差の配列のi列j行、kチャンネルの値\n",
    "$M$  : 出力チャンネル数 ただし、$i−s<0$ または$i−s>N_{out,h}−1$ または$j−t<0$ または $j−t>N_{out,w}−1$ のとき $\\frac{\\partial L}{\\partial a_{(i-s)(j-t),m}}w_{s, t, k, m}=0$です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】2次元畳み込み後の出力サイズ\n",
    "畳み込みを行うと特徴マップのサイズが変化します。どのように変化するかは以下の数式から求められます。この計算を行う関数を作成してください。\n",
    "$$N_{h, out} = \\frac{N_{h, in} + 2P_h - F_h}{S_h} + 1$$\n",
    "$$N_{w, out} = \\frac{N_{w, in} + 2P_w - F_w}{S_w} + 1$$\n",
    "\n",
    "$N_{out}$  : 出力のサイズ（特徴量の数）\n",
    "\n",
    "$N_{in}$ : 入力のサイズ（特徴量の数）\n",
    "\n",
    "$P$ : ある方向へのパディングの数\n",
    "\n",
    "$F$ : フィルタのサイズ\n",
    "\n",
    "$S$ : ストライドのサイズ\n",
    "\n",
    "$h$ が高さ方向、$w$ が幅方向である"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_out(Nh_in, Nw_in, P, Fh, Fw, S):\n",
    "    Nh_out = ((Nh_in  + 2*P - Fh) / S) + 1\n",
    "    Nw_out = ((Nw_in + 2*P-Fw) / S) + 1\n",
    "    return int(Nh_out), int(Nw_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】最大プーリング層の作成\n",
    "最大プーリング層のクラスMaxPool2Dを作成してください。プーリング層は数式で表さない方が分かりやすい部分もありますが、数式で表すとフォワードプロパゲーションは以下のようになります。\n",
    "$$a_{i,j,k} = max_{(p,q)\\in P_{i,j}} x_{p,q,k}$$\n",
    "\n",
    "$P_{i,j}$  : i行j列への出力する場合の入力配列のインデックスの集合。 $S_h×S_w$ の範囲内の行$（p）$と列$（q）$\n",
    "\n",
    "$S_h,S_w$ : 高さ方向$（h）$と幅方向$（w）$のストライドのサイズ\n",
    "\n",
    "$(p,q)\\in P_{i,j}$ : $P_{i,j}$ に含まれる行$（p）$と列$（q）$のインデックス\n",
    "\n",
    "$a_{i,j,m}$ : 出力される配列のi行j列、kチャンネルの値\n",
    "\n",
    "$x_{p,q,k}$ : 入力の配列の$p$行$q$列、$k$チャンネルの値\n",
    "\n",
    "ある範囲の中でチャンネル方向の軸は残したまま最大値を計算することになります。\n",
    "\n",
    "バックプロパゲーションのためには、フォワードプロパゲーションのときの最大値のインデックス $(p,q)$ を保持しておく必要があります。フォワード時に最大値を持っていた箇所にそのままの誤差を流し、そこ以外には0を入れるためです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D:\n",
    "    \"\"\"\n",
    "    pooling層のクラス\n",
    "    2*2の正方行列内の最大値をリストに格納\n",
    "    リストをreshapeすることで行列として返す\n",
    "    \n",
    "    2*2の正方行列内の最大値のインデックスを縦横それぞれ別のリストに格納\n",
    "    要素が0の行列を作り、backwardで帰ってきた値を最大値のインデックスがあった場所へ代入\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self,A):\n",
    "        self.AB = A\n",
    "        pooling_list = []\n",
    "        self.h_list = []\n",
    "        self.w_list = []\n",
    "        for n in range(A.shape[0]):\n",
    "            for cha in range(A.shape[1]):\n",
    "                for h in range(0, 26, 2):\n",
    "                    for w in range(0, 26, 2):\n",
    "                        pooling_list = np.append(pooling_list, np.max(A[n, cha, h:h+2, w:w+2]))\n",
    "                        self.h_list.append(np.unravel_index(np.argmax(A[n, cha, h:h+2, w:w+2]), A[n, cha, h:h+2, w:w+2].shape)[0])\n",
    "                        self.w_list.append(np.unravel_index(np.argmax(A[n, cha, h:h+2, w:w+2]), A[n, cha, h:h+2, w:w+2].shape)[1])\n",
    "                        \n",
    "        pooing = pooling_list.reshape(A.shape[0], A.shape[1], 13, 13)\n",
    "        return pooing\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        d = np.zeros_like(self.AB)\n",
    "        i = 0\n",
    "        for n in range(dA.shape[0]):\n",
    "            for cha in range(dA.shape[1]):\n",
    "                for h in range(0, 26, 2):\n",
    "                    for w in range(0, 26, 2):\n",
    "                        d[n, cha, h:h+2, w:w+2][self.h_list[i]][self.w_list[i]] = dA.reshape(-1)[i]\n",
    "                        i+=1\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題4】（アドバンス課題）平均プーリングの作成\n",
    "平均プーリング層のクラスAveragePool2Dを作成してください。\n",
    "\n",
    "範囲内の最大値ではなく、平均値を出力とするプーリング層です。\n",
    "\n",
    "画像認識関係では最大プーリング層が一般的で、平均プーリングはあまり使われません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題5】平滑化\n",
    "平滑化するためのFlattenクラスを作成してください。\n",
    "\n",
    "フォワードのときはチャンネル、高さ、幅の3次元を1次元にreshapeします。その値は記録しておき、バックワードのときに再びreshapeによって形を戻します。\n",
    "\n",
    "この平滑化のクラスを挟むことで出力前の全結合層に適した配列を作ることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    def forward(self, X):\n",
    "        self.X_shape = X.shape\n",
    "        return X.reshape(X.shape[0], -1)\n",
    "    \n",
    "    def bakcward(self, dA):\n",
    "        return dA.reshape(self.X_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.検証\n",
    "\n",
    "### 【問題6】学習と推定\n",
    "作成したConv2dを使用してMNISTを学習・推定し、Accuracyを計算してください。\n",
    "\n",
    "精度は低くともまずは動くことを目指してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        init = initializer\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.w = init.W(n_nodes1, n_nodes2)\n",
    "        self.b = init.B(n_nodes2)\n",
    "    \n",
    "\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.z = X\n",
    "        self.a = X@self.w + self.b\n",
    "        \n",
    "        return self.a\n",
    "\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        dZ = dA @ self.w.T\n",
    "        self.lw = self.z.T @ dA\n",
    "        self.lb = np.sum(dA, axis=0)\n",
    "        \n",
    "        \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B  = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer_cnn:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, out_chanel, in_chanel, height, width):\n",
    "\n",
    "        W = self.sigma * np.random.randn(out_chanel, in_chanel, height, width)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, out_chanel):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B  = self.sigma * np.random.randn(out_chanel)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \n",
    "        layer.w = layer.w -  self.lr * layer.lw\n",
    "        layer.b = layer.b - self.lr*layer.lb\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, A):\n",
    "        exp_a = np.exp(A)\n",
    "        softmax_result = np.empty((A.shape[0], A.shape[1]))\n",
    "        exp_sum = np.sum(exp_a, axis=1)\n",
    "        for i in range(A.shape[0]):\n",
    "            softmax_result[i] = exp_a[i] / exp_sum[i]\n",
    "            \n",
    "        return softmax_result\n",
    "    \n",
    "    def backward(self, Z, Y):\n",
    "        \n",
    "        L_A = Z - Y\n",
    "        self.cross_entropy = -np.average(np.sum(Y*np.log(Z), axis=1))\n",
    "        \n",
    "        \n",
    "        return L_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def forward(self, X):\n",
    "        self.A = X\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def backward(self, Z):\n",
    "        \n",
    "        return Z * np.maximum(np.sign(self.A), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 1, 28, 28)\n",
    "X_test = X_test.reshape(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#スクラッチが悪いせいで時間がかかるため、データ数を減らす\n",
    "X_train_min = X_train[:3000]\n",
    "\n",
    "t_train_min = t_train_one_hot[:3000]\n",
    "\n",
    "X_test_min = X_test[:500]\n",
    "\n",
    "t_test_min = t_test_one_hot[:500]\n",
    "\n",
    "conv2d = Conv2d(SimpleInitializer_cnn(0.01), 3, 1, 3, 3, SGD(0.1))\n",
    "maxpool = MaxPool2D()\n",
    "relu = Relu()\n",
    "fc1 = FC(507, 10, SimpleInitializer(0.01), SGD(0.1))\n",
    "softmax = Softmax()\n",
    "\n",
    "flat = Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09\n"
     ]
    }
   ],
   "source": [
    "test_A = conv2d.forward(X_test_min)\n",
    "test_A_2 = relu.forward(test_A)\n",
    "test_A_3 = maxpool.forward(test_A_2)\n",
    "\n",
    "test_A_4 = flat.forward(test_A_3)\n",
    "test_A_5 = fc1.forward(test_A_4)\n",
    "\n",
    "test_A_6 = softmax.forward(test_A_5)\n",
    "\n",
    "y = np.argmax(test_A_6 , axis=1)\n",
    "\n",
    "print(accuracy_score(t_test[:500], y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題7】（アドバンス課題）LeNet\n",
    "CNNで画像認識を行う際は、フィルタサイズや層の数などを１から考えるのではなく、有名な構造を利用することが一般的です。現在では実用的に使われることはありませんが、歴史的に重要なのは1998年の LeNet です。この構造を再現してMNISTに対して動かし、Accuracyを計算してください。\n",
    "\n",
    "Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n",
    "\n",
    "※上記論文から引用\n",
    "\n",
    "サブサンプリングとは現在のプーリングに相当するものです。現代風に以下のように作ってみることにします。活性化関数も当時はシグモイド関数ですが、ReLUとします。\n",
    "\n",
    "畳み込み層　出力チャンネル数6、フィルタサイズ5×5、ストライド1\n",
    "ReLU\n",
    "最大プーリング\n",
    "畳み込み層　出力チャンネル数16、フィルタサイズ5×5、ストライド1\n",
    "ReLU\n",
    "最大プーリング\n",
    "平滑化\n",
    "全結合層　出力ノード数120\n",
    "ReLU\n",
    "全結合層　出力ノード数84\n",
    "ReLU\n",
    "全結合層　出力ノード数10\n",
    "ソフトマックス関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題8】（アドバンス課題）有名な画像認識モデルの調査\n",
    "CNNの代表的な構造としてははAlexNet(2012)、VGG16(2014)などがあります。こういったものはフレームワークで既に用意されていることも多いです。\n",
    "\n",
    "どういったものがあるか簡単に調べてまとめてください。名前だけでも見ておくと良いでしょう。\n",
    "\n",
    "《参考》\n",
    "\n",
    "Applications - Keras Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題9】出力サイズとパラメータ数の計算\n",
    "CNNモデルを構築する際には、全結合層に入力する段階で特徴量がいくつになっているかを事前に計算する必要があります。\n",
    "\n",
    "また、巨大なモデルを扱うようになると、メモリや計算速度の関係でパラメータ数の計算は必須になってきます。フレームワークでは各層のパラメータ数を表示させることが可能ですが、意味を理解していなくては適切な調整が行えません。\n",
    "\n",
    "以下の3つの畳み込み層の出力サイズとパラメータ数を計算してください。パラメータ数についてはバイアス項も考えてください。\n",
    "\n",
    "1.\n",
    "\n",
    "入力サイズ : 144×144, 3チャンネル\n",
    "フィルタサイズ : 3×3, 6チャンネル\n",
    "ストライド : 1\n",
    "パディング : なし\n",
    "2.\n",
    "\n",
    "入力サイズ : 60×60, 24チャンネル\n",
    "フィルタサイズ : 3×3, 48チャンネル\n",
    "ストライド　: 1\n",
    "パディング : なし\n",
    "3.\n",
    "\n",
    "入力サイズ : 20×20, 10チャンネル\n",
    "フィルタサイズ: 3×3, 20チャンネル\n",
    "ストライド : 2\n",
    "パディング : なし\n",
    "＊最後の例は丁度良く畳み込みをすることができない場合です。フレームワークでは余ったピクセルを見ないという処理が行われることがあるので、その場合を考えて計算してください。端が欠けてしまうので、こういった設定は好ましくないという例です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題10】（アドバンス課題）フィルタサイズに関する調査\n",
    "畳み込み層にはフィルタサイズというハイパーパラメータがありますが、2次元畳み込み層において現在では3×3と1×1の使用が大半です。以下のそれぞれを調べたり、自分なりに考えて説明してください。\n",
    "\n",
    "7×7などの大きめのものではなく、3×3のフィルタが一般的に使われる理由\n",
    "高さや幅方向を持たない1×1のフィルタの効果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
